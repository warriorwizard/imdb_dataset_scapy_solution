{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**ASSIGNNMENT 1:-**\n1) Take sample dataset of IMDB,\n\n2) Remove special chracters and hyperlinks from text.\n\n3) Using Spacy Find the user names,City mentioned in the top 5000 records and store in list and add another column to list called Entity like if name is name of\nperson then new column should contain \"Person\" if name is name of City then new column should be \"City\"\n\n4) Store above data in DataFrame and convert  it t o JSON and  return that JSON from function.\n\n5) Create web app through Flask and create POST API and post API will take Name(text) as input parameter and should return is it City or Person.","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-24T18:05:39.203697Z","iopub.execute_input":"2022-08-24T18:05:39.204071Z","iopub.status.idle":"2022-08-24T18:05:39.223821Z","shell.execute_reply.started":"2022-08-24T18:05:39.204040Z","shell.execute_reply":"2022-08-24T18:05:39.222775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:39.296875Z","iopub.execute_input":"2022-08-24T18:05:39.297920Z","iopub.status.idle":"2022-08-24T18:05:40.563489Z","shell.execute_reply.started":"2022-08-24T18:05:39.297875Z","shell.execute_reply":"2022-08-24T18:05:40.562559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:40.567198Z","iopub.execute_input":"2022-08-24T18:05:40.567486Z","iopub.status.idle":"2022-08-24T18:05:40.595773Z","shell.execute_reply.started":"2022-08-24T18:05:40.567460Z","shell.execute_reply":"2022-08-24T18:05:40.594584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:40.597363Z","iopub.execute_input":"2022-08-24T18:05:40.597723Z","iopub.status.idle":"2022-08-24T18:05:40.678572Z","shell.execute_reply.started":"2022-08-24T18:05:40.597681Z","shell.execute_reply":"2022-08-24T18:05:40.677496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -U pip setuptools wheel\n# !pip install -U 'spacy[cuda113]'\n# !python -m spacy download en_core_web_sm","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:40.680142Z","iopub.execute_input":"2022-08-24T18:05:40.680603Z","iopub.status.idle":"2022-08-24T18:05:40.685120Z","shell.execute_reply.started":"2022-08-24T18:05:40.680570Z","shell.execute_reply":"2022-08-24T18:05:40.684154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install spacy\nimport spacy\nspacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:40.688385Z","iopub.execute_input":"2022-08-24T18:05:40.689072Z","iopub.status.idle":"2022-08-24T18:05:41.417399Z","shell.execute_reply.started":"2022-08-24T18:05:40.689031Z","shell.execute_reply":"2022-08-24T18:05:41.416470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install hypothesis\n# !python -c \"import os; import spacy; print(os.path.dirname(spacy.__file__))\"\n# !pip install -r path/to/requirements.txt\n# !python -m pytest --pyargs spacy","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:41.419035Z","iopub.execute_input":"2022-08-24T18:05:41.419402Z","iopub.status.idle":"2022-08-24T18:05:41.424145Z","shell.execute_reply.started":"2022-08-24T18:05:41.419367Z","shell.execute_reply":"2022-08-24T18:05:41.423171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.head()\n# import nltk\n# nltk.download('omw-1.4')\n# from nltk.tokenize import RegexpTokenizer\n# from nltk.stem import WordNetLemmatizer,PorterStemmer\n# from nltk.corpus import stopwords\n# import re\n# lemmatizer = WordNetLemmatizer()\n# stemmer = PorterStemmer()\n\n# def preprocess(sentence):\n#     sentence=str(sentence)\n#     sentence = sentence.lower()\n#     sentence=sentence.replace('{html}',\"\") \n#     cleanr = re.compile('<.*?>')\n#     cleantext = re.sub(cleanr, '', sentence)\n#     rem_url=re.sub(r'http\\S+', '',cleantext)\n#     rem_num = re.sub('[0-9]+', '', rem_url)\n#     tokenizer = RegexpTokenizer(r'\\w+')\n#     tokens = tokenizer.tokenize(rem_num)  \n#     filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n#     stem_words=[stemmer.stem(w) for w in filtered_words]\n#     lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n#     return \" \".join(filtered_words)\n# df['cleanText']=df['review'].map(lambda s:preprocess(s)) \n","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:41.425458Z","iopub.execute_input":"2022-08-24T18:05:41.426548Z","iopub.status.idle":"2022-08-24T18:05:41.435022Z","shell.execute_reply.started":"2022-08-24T18:05:41.426496Z","shell.execute_reply":"2022-08-24T18:05:41.434000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('after_clean_test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:41.436587Z","iopub.execute_input":"2022-08-24T18:05:41.437030Z","iopub.status.idle":"2022-08-24T18:05:42.589451Z","shell.execute_reply.started":"2022-08-24T18:05:41.436992Z","shell.execute_reply":"2022-08-24T18:05:42.588456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df=pd.read_csv('../input/after-clean-testcsv/after_clean_test.csv')\nnew_df.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:42.591701Z","iopub.execute_input":"2022-08-24T18:05:42.592380Z","iopub.status.idle":"2022-08-24T18:05:44.463187Z","shell.execute_reply.started":"2022-08-24T18:05:42.592337Z","shell.execute_reply":"2022-08-24T18:05:44.462288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean = pd.DataFrame(new_df,columns=['cleanText'])\ndf_clean.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:44.464456Z","iopub.execute_input":"2022-08-24T18:05:44.465305Z","iopub.status.idle":"2022-08-24T18:05:44.477701Z","shell.execute_reply.started":"2022-08-24T18:05:44.465266Z","shell.execute_reply":"2022-08-24T18:05:44.476713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean=df_clean[:5000]\ndf_clean.describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:44.479477Z","iopub.execute_input":"2022-08-24T18:05:44.479954Z","iopub.status.idle":"2022-08-24T18:05:44.498328Z","shell.execute_reply.started":"2022-08-24T18:05:44.479910Z","shell.execute_reply":"2022-08-24T18:05:44.497538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean['cleanText'][0]","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:44.499580Z","iopub.execute_input":"2022-08-24T18:05:44.499894Z","iopub.status.idle":"2022-08-24T18:05:44.509032Z","shell.execute_reply.started":"2022-08-24T18:05:44.499862Z","shell.execute_reply":"2022-08-24T18:05:44.507948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import en_core_web_sm\nimport spacy\ntext = df_clean['cleanText'][0]\nnlp = en_core_web_sm.load()\ndoc = nlp(text)\nprint(doc)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:44.510750Z","iopub.execute_input":"2022-08-24T18:05:44.511161Z","iopub.status.idle":"2022-08-24T18:05:45.140618Z","shell.execute_reply.started":"2022-08-24T18:05:44.511127Z","shell.execute_reply":"2022-08-24T18:05:45.139579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = []\nfor token in doc:\n    features.append({'token' : token.text, 'pos' : token.pos_})","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:45.144375Z","iopub.execute_input":"2022-08-24T18:05:45.144692Z","iopub.status.idle":"2022-08-24T18:05:45.149696Z","shell.execute_reply.started":"2022-08-24T18:05:45.144664Z","shell.execute_reply":"2022-08-24T18:05:45.148648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fdf = pd.DataFrame(features)\nfdf.head(len(fdf))","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:45.151070Z","iopub.execute_input":"2022-08-24T18:05:45.151664Z","iopub.status.idle":"2022-08-24T18:05:45.169681Z","shell.execute_reply.started":"2022-08-24T18:05:45.151630Z","shell.execute_reply":"2022-08-24T18:05:45.168785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfinal=[]\n\nimport spacy\nn=5000\nfor i in range(n):\n    text = df_clean['cleanText'][i]\n    nlp = en_core_web_sm.load()\n    doc = nlp(text)\n#     print(doc)\n\n\n    english_nlp = spacy.load('en_core_web_sm')\n    text = doc\n    spacy_parser = english_nlp(text)\n    print(f'{int((i/n)*100)+1} percent',end='  ')\n    for entity in spacy_parser.ents:\n        if entity.label_ == 'GPE':\n            final.append({'entity': entity.text, 'type':'city/country'})\n        if entity.label_ == 'PERSON':\n            final.append({'entity': entity.text, 'type':'person'})\n#         print(f'Found: {entity.text} of type: {entity.label_}')\n#     print('-----------------------------------------------')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:05:45.172881Z","iopub.execute_input":"2022-08-24T18:05:45.173193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"entity_relation= pd.DataFrame(final)\nentity_relation.head(len(entity_relation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"entity_relation.to_json('entity_relation_5000.json')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}